{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL score calculator to determine if it is malicious or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models make use of Word2Vec (CBOW) and FastText for certain components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from urllib.parse import urlparse,urlsplit\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input data :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(url_list):\n",
    "    new_df=pd.DataFrame(columns=[\"scheme\",\"netloc\",\"path\",\"query\",\"fragment\"])\n",
    "    for url in url_list:\n",
    "    #     test = [urlsplit(url).scheme, urlsplit(url).netloc, urlsplit(url).path, urlsplit(url).query, urlsplit(url).fragment]\n",
    "        split_result=urlsplit(url)\n",
    "        scheme = split_result.scheme\n",
    "        netloc = split_result.netloc\n",
    "        path = split_result.path\n",
    "        query = split_result.query\n",
    "        new_df =  new_df.append({'scheme': scheme, 'netloc': netloc, 'path': path, 'query': query}, ignore_index= True)\n",
    "        new_df.fillna(0,inplace=True)\n",
    "    return (new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorise using word2vec (cbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fiddling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer_netloc(new_df):\n",
    "    \n",
    "    model1 = Word2Vec.load(\"w2v_url\")\n",
    "    \n",
    "    \n",
    "    splitr1= new_df[\"netloc\"]\n",
    "\n",
    "    splitr1_1= [str(val).split(\".\") for val in splitr1.tolist()]\n",
    "    \n",
    "    model1.build_vocab(splitr1_1, update=True)\n",
    "    model1.train(splitr1_1, total_examples=len(splitr1_1), epochs=1)\n",
    "\n",
    "    \n",
    "    holder1=[]\n",
    "    vec_df1= pd.DataFrame()\n",
    "    val=[0 for i in range(100)]\n",
    "    for i in range(len(splitr1_1)):\n",
    "        for j in range(len(splitr1_1[i])):\n",
    "            val += model1[splitr1_1[i][j]]\n",
    "        holder1.append(val.tolist())\n",
    "        val *=0\n",
    "    vec_df1=vec_df1.append(holder1)   \n",
    "    \n",
    "    \n",
    "    return(vec_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer_path(new_df):\n",
    "    \n",
    "    model2 = Word2Vec.load(\"w2v_path\")\n",
    "    \n",
    "    \n",
    "    splitr2= new_df[\"path\"]\n",
    "    \n",
    "\n",
    "\n",
    "    splitr2_1= [str(val).split(\"/\") for val in splitr2.tolist()]\n",
    "  \n",
    "\n",
    "    model2.build_vocab(splitr2_1, update=True)\n",
    "    model2.train(splitr2_1, total_examples=len(splitr2_1), epochs=1)\n",
    "    \n",
    "\n",
    "    holder2=[]\n",
    "    vec_df2= pd.DataFrame()\n",
    "    val=[0 for i in range(100)]\n",
    "    for i in range(len(splitr2_1)):\n",
    "        for j in range(len(splitr2_1[i])):\n",
    "            val += model2[splitr2_1[i][j]]\n",
    "        holder2.append(val.tolist())\n",
    "        val *=0\n",
    "    vec_df2=vec_df2.append(holder2) \n",
    "    \n",
    "    \n",
    "    return(vec_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer_query(new_df):\n",
    "    \n",
    "    model3 = Word2Vec.load(\"w2v_query\")\n",
    "    \n",
    "    splitr3= new_df[\"query\"]\n",
    "\n",
    "    splitr3_1= [str(val).split(\"=\") for val in splitr3.tolist()]\n",
    "\n",
    "\n",
    "    model3.build_vocab(splitr3_1, update=True)\n",
    "    model3.train(splitr3_1, total_examples=len(splitr3_1), epochs=1)\n",
    "\n",
    "    holder3=[]\n",
    "    vec_df3= pd.DataFrame()\n",
    "    val=[0 for i in range(100)]\n",
    "    for i in range(len(splitr3_1)):\n",
    "        for j in range(len(splitr3_1[i])):\n",
    "            val += model3[splitr3_1[i][j]]\n",
    "        holder3.append(val.tolist())\n",
    "        val *=0\n",
    "    vec_df3=vec_df3.append(holder3) \n",
    "    \n",
    "    \n",
    "    return(vec_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_df(new_df):\n",
    "    pool = ThreadPool(processes=2)\n",
    "    new_df=new_df\n",
    "    async_result1 = pool.apply_async(vectorizer_netloc, args=(new_df,))\n",
    "    async_result2 = pool.apply_async(vectorizer_path, args=(new_df,))\n",
    "    async_result3 = pool.apply_async(vectorizer_query, args=(new_df,))# tuple of args for foo\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    vec_df1 = async_result1.get()\n",
    "    vec_df2 = async_result2.get()\n",
    "    vec_df3 = async_result3.get()\n",
    "    \n",
    "    tester_df_batch=pd.DataFrame()\n",
    "    tester_df_batch=pd.concat([vec_df1,vec_df2,vec_df3], axis=1)\n",
    "    return(tester_df_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL score prediction, update, endpoint mischief scorer :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## url score updater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_avg_for_updated_url_score (eps,model):\n",
    "    switch = []\n",
    "    for key in eps.keys():\n",
    "        for val in range(len(eps[key])):\n",
    "            switch+=[(eps[key][val],key,calc_endpoint_score(model.predict(convert_to_df(splitter(eps[key]))).tolist()))]\n",
    "\n",
    "    y =pd.DataFrame(switch, columns=['url','ip','eps_score'])\n",
    "\n",
    "    grouped_sum = y.groupby('url').sum()\n",
    "    grouped_size = y.groupby('url').size()\n",
    "    # grouped_size['a']\n",
    "\n",
    "\n",
    "    unique_urls = list(y['url'].unique())\n",
    "    avg_scores_of_endpoints_hitting_the_same_url =[]\n",
    "    for url in unique_urls :\n",
    "        avg_scores_of_endpoints_hitting_the_same_url += [{url:grouped_sum['eps_score'][url]/grouped_size[url]}]\n",
    "\n",
    "    return(avg_scores_of_endpoints_hitting_the_same_url,unique_urls)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## endpoint score calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_endpoint_score(score):\n",
    "    day_score = float()\n",
    "    bad_counter = int()\n",
    "    for sc in score:\n",
    "\n",
    "        if sc[0] > 0.5:\n",
    "            day_score += sc[0]\n",
    "            bad_counter += 1\n",
    "        else:\n",
    "            day_score += sc[0]\n",
    "\n",
    "    bad_counter\n",
    "    ep_w = (day_score/len(score))*(1.0-(1-(bad_counter/len(score))))\n",
    "    return(ep_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## url name_vector relatedness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_relatedness(url):\n",
    "    w_ = urlsplit(url)\n",
    "    w_netloc = w_.netloc.split('.')\n",
    "    w_path = w_.path.split('.')\n",
    "    \n",
    "    if(w_netloc[0]==''):\n",
    "\n",
    "        related_word=w_path\n",
    "        if(related_word[0] == 'www'):\n",
    "            related_word_finder = related_word[1]\n",
    "\n",
    "        else:\n",
    "            related_word_finder = related_word[0]\n",
    "\n",
    "    elif(w_netloc[0]=='www'):\n",
    "        related_word_finder = w_netloc[1]\n",
    "\n",
    "    else:\n",
    "        related_word_finder = w_netloc[0]\n",
    "        \n",
    "        \n",
    "    FastText_model = FastText.load('ft_.model')\n",
    "    relatedness_vector = FastText_model.wv.most_similar_cosmul(related_word_finder)\n",
    "    rv =np.mean([i[1] for i in relatedness_vector])\n",
    "    return(rv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## updating url scores based on relatedness and endpoint scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_score_recalculator(val_avg, unique_urls, url_scores):\n",
    "    val_avg = val_avg\n",
    "    unique_urls = unique_urls\n",
    "    url_scores = [url_scores[i][0] for i in range(len(url_scores))]\n",
    "    avg_list = [float(str(*list(val_avg[i].values()))) for i in range(len(val_avg))]\n",
    "    \n",
    "    rv =[vector_relatedness(url) for url in unique_urls]\n",
    "    m_rv_url_score = np.add(np.multiply(url_scores,0.85),np.multiply(rv,0.1))\n",
    "    \n",
    "    m_avg_list = np.multiply(avg_list,0.05)\n",
    "    \n",
    "    updated_scores = np.subtract(m_rv_url_score,m_avg_list)\n",
    "    updated_url_score_dict = [{unique_urls[i]:updated_scores[i]} for i in range(len(unique_urls))]\n",
    "    return(updated_url_score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\adessai\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\adessai\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\adessai\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\adessai\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\adessai\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From C:\\Users\\adessai\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\adessai\\anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\adessai\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adessai\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "C:\\Users\\adessai\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\adessai\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\adessai\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 43.3 s\n"
     ]
    }
   ],
   "source": [
    "eps = {'192.168.6.6.1':[\"https://www.yagle.com/abcd/?=123\",\"https://www.google.com\",\"www.bitsadmin.com/\",\"https://www.yagle.com/abcd/?=123\",\"https://www.google.com\",\"www.bitsadmin.com/\",\"http://zastapiony.piklamp.nl/military.xbel?face=jlthEvD&oil=70UfcEdppE&similar=0T9GXVd&plane=&never=XzUI&size=h-S&building=3NHL3LH4j&play=Ke9C4&over=&pattern=gKK\",\"https://www.google.com\"],\n",
    "      '0.0.0.1':[\"https://www.yagle.com/abcd/?=123\",\"https://www.google.com\",\"www.bitsadmin.com/\"]}\n",
    "\n",
    "### on assuming an input of the above format to the code\n",
    "\n",
    "model = load_model('current.best__std.hdf5')\n",
    "val_avg, unique_urls = return_avg_for_updated_url_score(eps,model)\n",
    "\n",
    "final_vec = convert_to_df(splitter(eps['192.168.6.6.1']))##\n",
    "url_scores = model.predict(convert_to_df(splitter(unique_urls))).tolist()\n",
    "updated_url_score_dict = url_score_recalculator(val_avg, unique_urls, url_scores)\n",
    "\n",
    "score =  model.predict(final_vec).tolist()\n",
    "ep_sc = calc_endpoint_score(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Vectors for URLs of '192.168.6.6.1' (3 x 300) :           0         1         2         3         4         5         6   \\\n",
      "0  1.458456 -3.254446  1.979042 -0.257234 -2.897713  1.217711 -1.651971   \n",
      "1  3.578903 -5.270251  2.364378  0.229436 -3.393633  1.455947 -2.137968   \n",
      "2 -0.145892 -0.154081 -0.383800 -0.552012 -1.168146 -0.070009  0.452086   \n",
      "3  1.458456 -3.254446  1.979042 -0.257234 -2.897713  1.217711 -1.651971   \n",
      "4  3.578903 -5.270251  2.364378  0.229436 -3.393633  1.455947 -2.137968   \n",
      "5 -0.145892 -0.154081 -0.383800 -0.552012 -1.168146 -0.070009  0.452086   \n",
      "6  0.909108 -0.324216 -0.405326 -0.360043 -0.810911 -0.161119 -0.166233   \n",
      "7  3.578903 -5.270251  2.364378  0.229436 -3.393633  1.455947 -2.137968   \n",
      "\n",
      "         7         8         9   ...        90        91        92        93  \\\n",
      "0  0.164570 -1.647398 -1.130620  ...  0.199735 -0.440787 -0.280912  0.482175   \n",
      "1  0.025913 -2.056639 -2.126576  ...  0.211939 -0.424193 -0.261291  0.470320   \n",
      "2  0.525980  1.194449 -1.202422  ...  0.211939 -0.424193 -0.261291  0.470320   \n",
      "3  0.164570 -1.647398 -1.130620  ...  0.199735 -0.440787 -0.280912  0.482175   \n",
      "4  0.025913 -2.056639 -2.126576  ...  0.211939 -0.424193 -0.261291  0.470320   \n",
      "5  0.525980  1.194449 -1.202422  ...  0.211939 -0.424193 -0.261291  0.470320   \n",
      "6  0.715379  0.619499 -1.033033  ... -0.256701 -0.290109 -0.292599  0.218603   \n",
      "7  0.025913 -2.056639 -2.126576  ...  0.211939 -0.424193 -0.261291  0.470320   \n",
      "\n",
      "         94        95        96        97        98        99  \n",
      "0 -1.150446  0.503363 -0.022905 -0.048369  0.052436  1.149770  \n",
      "1 -1.098585  0.471811  0.010624 -0.075901  0.035317  1.103188  \n",
      "2 -1.098585  0.471811  0.010624 -0.075901  0.035317  1.103188  \n",
      "3 -1.150446  0.503363 -0.022905 -0.048369  0.052436  1.149770  \n",
      "4 -1.098585  0.471811  0.010624 -0.075901  0.035317  1.103188  \n",
      "5 -1.098585  0.471811  0.010624 -0.075901  0.035317  1.103188  \n",
      "6 -0.685733  0.460270 -0.386835  0.412884  0.171025  0.601937  \n",
      "7 -1.098585  0.471811  0.010624 -0.075901  0.035317  1.103188  \n",
      "\n",
      "[8 rows x 300 columns]\n",
      "\n",
      "Scores for all unique URLs not bound by the endpoint: \n",
      " [[0.031066881492733955], [0.06353820115327835], [0.9999127388000488], [0.9910424947738647]]\n",
      "\n",
      "Updated URL scores to dict: \n",
      " [{'https://www.yagle.com/abcd/?=123': 0.08120791900592546}, {'https://www.google.com': 0.1421347633667756}, {'www.bitsadmin.com/': 0.9028305777317533}, {'http://zastapiony.piklamp.nl/military.xbel?face=jlthEvD&oil=70UfcEdppE&similar=0T9GXVd&plane=&never=XzUI&size=h-S&building=3NHL3LH4j&play=Ke9C4&over=&pattern=gKK': 0.8930600546253845}]\n",
      "\n",
      "Scores for URLs of '192.168.6.6.1' endpoint: \n",
      " [[0.03081420063972473], [0.06119498610496521], [0.999914288520813], [0.03081420063972473], [0.06119498610496521], [0.999914288520813], [0.9931920766830444], [0.06119498610496521]]\n",
      "\n",
      "Endpoint ('192.168.6.6.1') score :  0.15179221937432885\n"
     ]
    }
   ],
   "source": [
    "print(\"Endpoint Vectors for URLs of '192.168.6.6.1' (3 x 300) : \",final_vec)\n",
    "print(\"\\nScores for all unique URLs not bound by the endpoint: \\n\",url_scores)\n",
    "print(\"\\nUpdated URL scores to dict: \\n\",updated_url_score_dict)\n",
    "print(\"\\nScores for URLs of '192.168.6.6.1' endpoint: \\n\",score)\n",
    "print(\"\\nEndpoint ('192.168.6.6.1') score : \",ep_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## future updates would make use of incremental learning structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

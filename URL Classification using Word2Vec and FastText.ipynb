{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL score calculator to determine if it is malicious or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models make use of Word2Vec (CBOW) and FastText for certain components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from keras.models import load_model\n",
    "from urllib.parse import urlparse,urlsplit\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, FastText  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input data :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(url_list):\n",
    "    new_df=pd.DataFrame(columns=[\"scheme\",\"netloc\",\"path\",\"query\",\"fragment\"])\n",
    "    for url in url_list:\n",
    "    #     test = [urlsplit(url).scheme, urlsplit(url).netloc, urlsplit(url).path, urlsplit(url).query, urlsplit(url).fragment]\n",
    "        split_result=urlsplit(url)\n",
    "        scheme = split_result.scheme\n",
    "        netloc = split_result.netloc\n",
    "        path = split_result.path\n",
    "        query = split_result.query\n",
    "        new_df =  new_df.append({'scheme': scheme, 'netloc': netloc, 'path': path, 'query': query}, ignore_index= True)\n",
    "        new_df.fillna(0,inplace=True)\n",
    "    return (new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorise using word2vec (cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(new_df):\n",
    "    \n",
    "    model1 = Word2Vec.load(\"w2v_url\")\n",
    "    model2 = Word2Vec.load(\"w2v_path\")\n",
    "    model3 = Word2Vec.load(\"w2v_query\")\n",
    "    \n",
    "    \n",
    "    splitr1= new_df[\"netloc\"]\n",
    "    splitr2= new_df[\"path\"]\n",
    "    splitr3= new_df[\"query\"]\n",
    "\n",
    "    splitr1_1= [str(val).split(\".\") for val in splitr1.tolist()]\n",
    "    splitr2_1= [str(val).split(\"/\") for val in splitr2.tolist()]\n",
    "    splitr3_1= [str(val).split(\"=\") for val in splitr3.tolist()]\n",
    "    print(len(splitr1_1),len(splitr2_1),len(splitr3_1))\n",
    "    \n",
    "    model1.build_vocab(splitr1_1, update=True)\n",
    "    model1.train(splitr1_1, total_examples=len(splitr1_1), epochs=1)\n",
    "\n",
    "    model2.build_vocab(splitr2_1, update=True)\n",
    "    model2.train(splitr2_1, total_examples=len(splitr1_1), epochs=1)\n",
    "\n",
    "    model3.build_vocab(splitr3_1, update=True)\n",
    "    model3.train(splitr3_1, total_examples=len(splitr1_1), epochs=1)\n",
    "    \n",
    "    holder1=[]\n",
    "    vec_df1= pd.DataFrame()\n",
    "    val=[0 for i in range(100)]\n",
    "    for i in range(len(splitr1_1)):\n",
    "        for j in range(len(splitr1_1[i])):\n",
    "            val += model1[splitr1_1[i][j]]\n",
    "        holder1.append(val.tolist())\n",
    "        val *=0\n",
    "    vec_df1=vec_df1.append(holder1)   \n",
    "    vec_df1.head()\n",
    "\n",
    "    holder2=[]\n",
    "    vec_df2= pd.DataFrame()\n",
    "    val=[0 for i in range(100)]\n",
    "    for i in range(len(splitr2_1)):\n",
    "        for j in range(len(splitr2_1[i])):\n",
    "            val += model2[splitr2_1[i][j]]\n",
    "        holder2.append(val.tolist())\n",
    "        val *=0\n",
    "    vec_df2=vec_df2.append(holder2) \n",
    "    vec_df2.head()\n",
    "\n",
    "    holder3=[]\n",
    "    vec_df3= pd.DataFrame()\n",
    "    val=[0 for i in range(100)]\n",
    "    for i in range(len(splitr3_1)):\n",
    "        for j in range(len(splitr3_1[i])):\n",
    "            val += model3[splitr3_1[i][j]]\n",
    "        holder3.append(val.tolist())\n",
    "        val *=0\n",
    "    vec_df3=vec_df3.append(holder3) \n",
    "    vec_df3.head()\n",
    "    \n",
    "    \n",
    "    tester_df_batch=pd.DataFrame()\n",
    "    tester_df_batch=pd.concat([vec_df1,vec_df2,vec_df3], axis=1)\n",
    "    \n",
    "    return(tester_df_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL score prediction, update, endpoint mischief scorer :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## url score updater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_avg_for_updated_url_score ():\n",
    "    switch = []\n",
    "    for key in eps.keys():\n",
    "        for val in range(len(eps[key])):\n",
    "            switch+=[(eps[key][val],key,calc_endpoint_score(model.predict(vectorizer(splitter(eps[key]))).tolist()))]\n",
    "\n",
    "    y =pd.DataFrame(switch, columns=['url','ip','eps_score'])\n",
    "\n",
    "    grouped_sum = y.groupby('url').sum()\n",
    "    grouped_size = y.groupby('url').size()\n",
    "    # grouped_size['a']\n",
    "\n",
    "\n",
    "    unique_urls = list(y['url'].unique())\n",
    "    avg_scores_of_endpoints_hitting_the_same_url =[]\n",
    "    for url in unique_urls :\n",
    "        avg_scores_of_endpoints_hitting_the_same_url += [{url:grouped_sum['eps_score'][url]/grouped_size[url]}]\n",
    "\n",
    "    return(avg_scores_of_endpoints_hitting_the_same_url)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## endpoint score calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_endpoint_score(score):\n",
    "    day_score = float()\n",
    "    bad_counter = int()\n",
    "    for sc in score:\n",
    "\n",
    "        if sc[0] > 0.5:\n",
    "            day_score += sc[0]\n",
    "            bad_counter += 1\n",
    "        else:\n",
    "            day_score += sc[0]\n",
    "\n",
    "    bad_counter\n",
    "    ep_w = (day_score/len(score))*(1.0-(1-(bad_counter/len(score))))\n",
    "    return(ep_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## url name_vector relatedness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_relatedness(url):\n",
    "    w_ = urlsplit(url)\n",
    "    w_netloc = w_.netloc.split('.')\n",
    "    w_path = w_.path.split('.')\n",
    "    \n",
    "    if(w_netloc[0]==''):\n",
    "\n",
    "        related_word=w_path\n",
    "        if(related_word[0] == 'www'):\n",
    "            related_word_finder = related_word[1]\n",
    "\n",
    "        else:\n",
    "            related_word_finder = related_word[0]\n",
    "\n",
    "    elif(w_netloc[0]=='www'):\n",
    "        related_word_finder = w_netloc[1]\n",
    "\n",
    "    else:\n",
    "        related_word_finder = w_netloc[0]\n",
    "        \n",
    "        \n",
    "    FastText_model = FastText.load('ft_.model')\n",
    "    relatedness_vector = FastText_model.wv.most_similar_cosmul(related_word_finder)\n",
    "    rv =np.mean([i[1] for i in relatedness_vector])\n",
    "    return(rv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## updating url scores based on relatedness and endpoint scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_score_recalculator(val_avg, unique_urls, url_scores):\n",
    "    val_avg = val_avg\n",
    "    unique_urls = unique_urls\n",
    "    url_scores = url_scores\n",
    "    avg_list = [float(str(*list(val_avg[i].values()))) for i in range(len(val_avg))]\n",
    "\n",
    "    rv =[vector_relatedness(url) for url in unique_urls]\n",
    "    m_rv_url_score = np.add(np.multiply(url_scores,0.85),np.multiply(rv,0.1))\n",
    "    m_avg_list = np.multiply(avg_list,0.05)\n",
    "\n",
    "    updated_scores = np.add(m_rv_url_score,m_avg_list)\n",
    "    updated_url_score_dict = [{unique_urls[i]:updator[i]} for i in len(unique_urls)]\n",
    "    return(updated_url_score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = {'192.168.6.6.1':[\"http://zastapiony.piklamp.nl/military.xbel?face=jlthEvD&oil=70UfcEdppE&similar=0T9GXVd&plane=&never=XzUI&size=h-S&building=3NHL3LH4j&play=Ke9C4&over=&pattern=gKK\",\"https://www.google.com\"],\n",
    "      '0.0.0.1':[\"https://www.yagle.com/abcd/?=123\",\"https://www.google.com\",\"www.bitsadmin.com/\"]}\n",
    "\n",
    "### on assuming an input of the above format to the code\n",
    "\n",
    "model = load_model('current.best__std.hdf5')\n",
    "val_avg, unique_urls = return_avg_for_updated_url_score(eps,model)\n",
    "\n",
    "final_vec = vectorizer(splitter(eps['0.0.0.1']))\n",
    "url_scores = model.predict(vectorizer(splitter(unique_urls))).tolist()\n",
    "updated_url_score_dict = url_score_recalculator(val_avg, unique_urls, url_scores)\n",
    "\n",
    "score =  model.predict(final_vec).tolist()\n",
    "ep_sc = calc_endpoint_score(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## future updates would make use of incremental learning structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
